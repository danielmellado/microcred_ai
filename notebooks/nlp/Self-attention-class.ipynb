{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TNiQAU0465Bo4iT2Ga2gt8Qt1zRrQzs9","timestamp":1758024365545}],"gpuType":"T4","authorship_tag":"ABX9TyMyQCV2nqTyPAO7oeczyt/n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e1PI7D8etv37"},"outputs":[],"source":["!pip install transformers matplotlib\n"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Use BERT, a classic Transformer encoder model\n","model_name = \"bert-base-uncased\" # there is also \"cased\", and generally, many other models\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name, output_attentions=True) # Pretrained"],"metadata":{"id":"DKv97-gDt5Ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try a different sentence at home\n","sentence = \"The bank robber drew a gun on the river bank.\"\n","inputs = tokenizer(sentence, return_tensors=\"pt\") # \"pt\" for PyTorch tensors\n","\n","# Get the model's output\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# The attention weights are in outputs.attentions\n","# First layer.\n","attention_layer_0 = outputs.attentions[0]\n","print(\"Attention tensor shape:\", attention_layer_0.shape)\n","# Shape: [batch_size, num_heads, sequence_length, sequence_length]"],"metadata":{"id":"0kAFZny6t6-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Average the attention weights across all heads, \"flatten\" into 2D\n","attention_heads_avg = attention_layer_0[0].mean(axis=0).numpy()\n","\n","# Get the tokens for labeling the plot\n","print(\"The sentence tokens vector: \")\n","print(inputs['input_ids'])\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","print(\"tokens: \")\n","print(tokens)\n","\n","\n","# Create a new figure and axis for plotting, setting figure size to 8x8 inches\n","fig, ax = plt.subplots(figsize=(8, 8))\n","\n","# Display the attention weights matrix as an image (heatmap) using the 'viridis' colormap\n","im = ax.imshow(attention_heads_avg, cmap=\"viridis\")\n","\n","# Set tick positions along x and y axes (one tick per token)\n","ax.set_xticks(np.arange(len(tokens)))\n","ax.set_yticks(np.arange(len(tokens)))\n","\n","# Label ticks with the actual token strings\n","ax.set_xticklabels(tokens)\n","ax.set_yticklabels(tokens)\n","\n","# Rotate x-axis tick labels for readability (45 degrees, right-aligned)\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","\n","# Add a colorbar to the figure to show the scale of attention values\n","fig.colorbar(im, ax=ax)\n","\n","# Add a descriptive title to the heatmap\n","ax.set_title(\"Self-Attention Heatmap (Layer 0)\")\n","\n","# Adjust layout so everything fits without overlapping\n","fig.tight_layout()\n","\n","# Render and display the plot\n","plt.show()"],"metadata":{"id":"HA35QOUouW13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Plot for Layer 11 ---\n","\n","# Extract attention from the last layer (index 11)\n","attention_layer_11 = outputs.attentions[11]\n","# Average the attention weights across all 12 heads for this layer\n","attention_heads_avg_11 = attention_layer_11[0].mean(axis=0).numpy()\n","\n","# Create a new figure and axis for the second plot\n","fig, ax = plt.subplots(figsize=(8, 8))\n","im = ax.imshow(attention_heads_avg_11, cmap=\"viridis\")\n","\n","# Set up labels and ticks (same as before, as tokens don't change)\n","ax.set_xticks(np.arange(len(tokens)))\n","ax.set_yticks(np.arange(len(tokens)))\n","ax.set_xticklabels(tokens)\n","ax.set_yticklabels(tokens)\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","fig.colorbar(im, ax=ax)\n","# IMPORTANT: Update the title for the new plot\n","ax.set_title(\"Self-Attention Heatmap (Layer 11)\")\n","fig.tight_layout()\n","plt.show()\n"],"metadata":{"id":"pPxRwKILNK9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Step 1: Import Necessary Libraries ##\n","\n","import torch\n","# Module for creating plots and graphs. We give it a shorter name, 'plt'.\n","import matplotlib.pyplot as plt\n","# Efficient numerical operations, especially for creating our range of sequence lengths.\n","import numpy as np\n","\n","\n","# ## Step 2: Setup the Computation Environment ##\n","\n","# In deep learning, you can run calculations on a CPU or a much faster GPU.\n","# This line checks if a CUDA-enabled GPU is available in the environment (like in Google Colab).\n","# If yes, it sets the 'device' to 'cuda'; otherwise, it falls back to the 'cpu'.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\") # Let's confirm which device we're using.\n","\n"],"metadata":{"id":"WIhIpha-wERN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Step 3: Define Parameters for our Attention Simulation ##\n","\n","# --- Attention Parameters ---\n","# Batch Size: How many sequences we process at once. We use 1 to focus purely on the effect of sequence length.\n","batch_size = 1\n","# Number of Heads: In Multi-Head Attention, the model splits its attention mechanism into multiple \"heads\"\n","# to focus on different parts of the input simultaneously. 8 is a common number.\n","num_heads = 8\n","# Model Dimension (d_model): The total size of the vector used to represent each token (word) in the sequence.\n","d_model = 512\n","# Head Dimension (d_head): The total model dimension is split evenly among the heads.\n","# So each head will work with smaller vectors of size 64 (512 / 8).\n","d_head = d_model // num_heads"],"metadata":{"id":"BA4-fywiMHdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ## Step 4: Create a Function to Isolate and Time the Core Operation ##\n","\n","# We create a function to keep our main loop clean. It will perform the timing for one sequence length.\n","def time_attention_operation(seq_len):\n","    \"\"\"Creates dummy tensors and accurately times the QKáµ€ matrix multiplication on the GPU.\"\"\"\n","\n","    # Create dummy Query (q) and Key (k) tensors with random data.\n","    # We don't need real words, just tensors of the correct SHAPE to measure performance.\n","    # The shape is (batch_size, num_heads, sequence_length, head_dimension).\n","    q = torch.randn(batch_size, num_heads, seq_len, d_head).to(device)\n","    k = torch.randn(batch_size, num_heads, seq_len, d_head).to(device)\n","\n","    # --- WARM-UP RUN ---\n","    # The very first time you run an operation on a GPU, it can be slower due to setup costs.\n","    # We run the operation once here without timing it to \"warm up\" the GPU.\n","    _ = torch.matmul(q, k.transpose(-2, -1))\n","    # GPU operations are asynchronous (the CPU doesn't wait for them to finish).\n","    # torch.cuda.synchronize() forces the CPU to wait until the GPU has finished all previous tasks.\n","    # This ensures our warm-up is complete before we start the real timing.\n","    torch.cuda.synchronize()\n","\n","    # --- TIMED RUN ---\n","    # For accurate GPU timing, we use torch.cuda.Event objects. They act like high-precision\n","    # stopwatches that live directly on the GPU, avoiding CPU-to-GPU communication delays.\n","    start_event = torch.cuda.Event(enable_timing=True)\n","    end_event = torch.cuda.Event(enable_timing=True)\n","\n","    # Record the \"start\" time.\n","    start_event.record()\n","\n","    # THIS IS THE CORE O(n^2) OPERATION!\n","    # `torch.matmul` performs matrix multiplication.\n","    # `k.transpose(-2, -1)` swaps the last two dimensions of the Key tensor.\n","    # This multiplies a (..., seq_len, d_head) matrix by a (..., d_head, seq_len) matrix,\n","    # resulting in a (..., seq_len, seq_len) attention score matrix. The cost scales with seq_len squared.\n","    attention_scores = torch.matmul(q, k.transpose(-2, -1))\n","\n","    # Record the \"end\" time.\n","    end_event.record()\n","\n","    # Synchronize again to ensure the timed operation is fully complete before we measure the time.\n","    torch.cuda.synchronize()\n","\n","    # Calculate and return the elapsed time in milliseconds.\n","    return start_event.elapsed_time(end_event)\n"],"metadata":{"id":"A2Z0sL5fMAwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Step 5: Run the Benchmark Across Different Sequence Lenths ##\n","\n","# --- Generate the sequence lengths we want to test ---\n","# np.arange(start, stop, step) creates an array of numbers.\n","# We'll test lengths from 256 up to 4096, in steps of 256. This gives us 16 data points.\n","sequence_lengths = np.arange(256, 4097, 256)\n","# Create an empty list to store the timing results from each run.\n","timings_ms = []\n","\n","print(\"Starting performance measurement with more data points...\")\n","# Loop through each sequence length we defined.\n","for seq_len in sequence_lengths:\n","    # Use a try/except block to handle potential errors.\n","    try:\n","        # Call our timing function for the current sequence length.\n","        elapsed_time_ms = time_attention_operation(seq_len)\n","        # Add the result to our list of timings.\n","        timings_ms.append(elapsed_time_ms)\n","        # Print the result so we can see the progress. The formatting just makes it align nicely.\n","        print(f\"Sequence Length: {seq_len:<5}, Time: {elapsed_time_ms:.4f} ms\")\n","    # If the tensors are too large, we can run out of GPU memory.\n","    # This `except` block catches that specific error so the program doesn't crash.\n","    except torch.cuda.OutOfMemoryError:\n","        print(f\"Sequence Length: {seq_len:<5}, FAILED: Out of GPU memory.\")\n","        # If we fail, we stop the loop and trim the lengths list to match the successful timings we have.\n","        sequence_lengths = sequence_lengths[:len(timings_ms)]\n","        break\n","\n","print(\"Measurement complete.\")"],"metadata":{"id":"xqcsoqPBL9Nk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## Step 6: Plot the Results ##\n","\n","# Create a new figure (the plotting window) and set its size for better readability.\n","plt.figure(figsize=(10, 6))\n","\n","# The main plotting command.\n","# It plots `sequence_lengths` on the x-axis and `timings_ms` on the y-axis.\n","# 'o-' is a style format: 'o' creates a circle marker at each data point,\n","# and '-' connects the markers with a solid line.\n","plt.plot(sequence_lengths, timings_ms, 'o-', label='Actual Measured Time')\n","\n","# Add labels to the graph.\n","plt.title('Self-Attention Cost: Measured Data', fontsize=16)\n","plt.xlabel('Sequence Length (n)', fontsize=12)\n","plt.ylabel('Execution Time (milliseconds)', fontsize=12)\n","\n","# Display the legend (the label we defined in the plt.plot command).\n","plt.legend()\n","# Add a grid to the background to make it easier to read the values.\n","plt.grid(True)\n","# Display the final plot.\n","plt.show()"],"metadata":{"id":"pTsRNqIHL6M8"},"execution_count":null,"outputs":[]}]}